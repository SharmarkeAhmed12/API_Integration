services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.0
    container_name: zookeeper2
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2182:2181"
    networks:
      - weather-net

  kafka:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka2
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
    networks:
      - weather-net

  topic-setup:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: kafka-topic-setup
    depends_on:
      - kafka
    env_file:
      - ./Secrets.env
    command: sh -c "sleep 20 && python setup_kafka_topics.py"
    networks:
      - weather-net
  # ===== AIRFLOW SERVICES (USING SQLITE FOR NOW) =====
  # We're delaying Postgres and using SQLite for Airflow's metadata DB.
  # This is OK for development/testing but not recommended for production.

  # Airflow Webserver: The dashboard you visit at localhost:8080
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    environment:
      AIRFLOW_HOME: /home/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/dags
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////home/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "false"
    depends_on:
      kafka:
        condition: service_started
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/home/airflow/dags
      - ./logs:/home/airflow/logs
      - ./datalake:/app/datalake
      - ./db:/app/db
      - ./Secrets.env:/home/airflow/app/Secrets.env
      - ./airflow_db:/home/airflow
    env_file:
      - ./Secrets.env
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com 2>/dev/null || true &&
      airflow webserver
      "
    networks:
      - weather-net
    restart: unless-stopped

  # Airflow Scheduler: The "manager" that decides when jobs run
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    environment:
      AIRFLOW_HOME: /home/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/dags
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////home/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "false"
    depends_on:
      - airflow-webserver
      - kafka
    volumes:
      - ./dags:/home/airflow/dags
      - ./logs:/home/airflow/logs
      - ./datalake:/app/datalake
      - ./db:/app/db
      - ./Secrets.env:/home/airflow/app/Secrets.env
      - ./airflow_db:/home/airflow
    env_file:
      - ./Secrets.env
    command: airflow scheduler
    networks:
      - weather-net
    restart: unless-stopped

networks:
  weather-net:
    driver: bridge
